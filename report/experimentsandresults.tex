\newpage
\section{Experiments and Results}
\label{sec:ExperimentsandResults}
In order to test the previously described \ac{TAD} approach several experiments
were performed, to find the optimal model for both the \ac{SISR} and \ac{IC}
tasks, to show the impact of the L1 ball on the model's robustness against
perturbations as well as evincing the feasibility of applying the \ac{TAD}
method in the video domain.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=14cm]{figures/model_adaptions}
	\caption{Model and Training adaptions during experiments in comparison
  to the baseline model by \cite{TAID}.}
  \label{fig:model_adaptions}
\end{figure}

As shown in \myfigref{fig:model_adaptions} a bunch of adjustments to the
baseline model (by \cite{TAID}) were tried for analysing the coherence between
the model complexity and its reconstruction performance. Thereby very small
architectures (6 layers) as well as comparable large architectures
(19 layers) were tested (baseline model has 10 layers), spanning
from $375.926$ to $1.299.126$ parameters. Since the model already is quite
shallow the removal of each layer had an impact on the resulting performance,
therefore especially the number of \textit{Resblocks} (two convolutional layers
and ReLU) has a huge impact on the reconstruction accuracy. Next to adaptions
to the baseline architecture completly new architectures have been tested, such
as networks without any residual pass (so no Resblocks) but convolutional layers
with either constant or varying (first increasing then decreasing) number of
filters (up to 256) have been used.
\newline
Next to several architectures the baseline model was improved by advancing the
training procedure. Next to enhancing the loss function as discussed in
\mysecref{sec:Approach_LF} instead of a constant an linearly annealing learning
rate was used, starting at $4*10^{-4}$ and annealing by factor $\gamma = 0.25$
after $20$, $100$ and again after $200$ training epochs. Adam optimizer was used
with $\beta = (0.9, 0.999)$, $\epsilon = 10^{-8}$, gradient clipping and  zero
weight decay.
\newline
To guarantee comparability to other super-resolution and colorization paper
in the image domain the model was trained using the DIV2K training dataset
(\cite{DIV2K}), while validated on the SET5 (\cite{SET5}), SET14 (\cite{SET14}),
BSDS100 (\cite{BSDS100}), URBAN100 (\cite{URBAN100}) and VDIV2K (\cite{DIV2K})
dataset. For similar reason for the video domain the model was pretrained using
DIV2K, actually trained on video clips from CDVL Database (Ntiaspen) and
validated on the widely known Vid4 dataset (Calendar, Foliage, Walk, City).
For improving generalization capabilities of the model and avoid overfitting
the image training data were also augmented (rotated, mirrored).
\newline
A complete list of the most important testing configurations and their results
can be found in the appendix.

\subsection{Impact of L1 Ball}
\label{sec:Experiments_EPS_BALL}



\subsection{Single-Image Super-Resolution}
\label{sec:Experiments_SISR}

\begin{enumerate}
\item different scales
\item very large scales
\item performance on other non-trained scales
\end{enumerate}

\subsection{Image Colorization}
\label{sec:Experiments_IC}

\subsection{Video Super-Resolution}
\label{sec:Experiments_VSR}

\begin{enumerate}
\item super-large scale for videos
\end{enumerate}

\subsection{Qualitative Improvements}
\label{sec:Experiments_QI}

\begin{enumerate}
\item gummibear image (TAD learns to downscale an image task-aware and thereby
can store information that are lost by simple downscaling (e.g. bilinear
interpolation, averaging colors). Therefore, possibly even image can restored
that are impossible to restore  using classic approaches, like exactly
similar gummibears.)
\end{enumerate}

% Describe the evaluation you did in a way, such that an independent researcher can repeat it. Cover the following questions:
% \begin{itemize}
%  \item \textit{What is the experimental setup and methodology?} Describe the setting of the experiments and give all the parameters in detail which you have used. Give a detailed account of how the experiment was conducted.
%  \item \textit{What are your results?} In this section, a \emph{clear description} of the results is given. If you produced lots of data, include only representative data here and put all results into the appendix.
% \end{itemize}
